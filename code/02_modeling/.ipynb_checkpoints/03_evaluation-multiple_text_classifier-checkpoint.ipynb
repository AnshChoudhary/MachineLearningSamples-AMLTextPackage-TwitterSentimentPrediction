{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of classifiers on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained pipeline models and evaluate an independent test data\n",
    "\n",
    "Similar scripts can be used when putting the model in production using REST APIs running on Azure ML deployment service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## SET YOUR OWN WORKING DIRECTORY\n",
    "working_dir = r\"<Set working directory>\"\n",
    "#working_dir = r\"D:\\Sentiment140_Classification\"\n",
    "\n",
    "## Set training and test files\n",
    "test_tweet_filename = os.path.join(working_dir, 'testing_text.csv')\n",
    "test_label_filename = os.path.join(working_dir, 'testing_label.csv')\n",
    "\n",
    "## Set model files\n",
    "sk_model_file = os.path.join(working_dir, 'sk_model.zip') \n",
    "keras_model_file = os.path.join(working_dir, 'keras_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: azureml-tatk\n",
      "Version: 0.0.687318\n",
      "Summary: Microsoft Azure Machine Learning Text Analytics Toolkit\n",
      "Home-page: https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_git/TATK\n",
      "Author: Microsoft Corporation\n",
      "Author-email: azml-tatk@microsoft.com\n",
      "License: UNKNOWN\n",
      "Location: c:\\users\\remoteuser\\appdata\\local\\amlworkbench\\python\\lib\\site-packages\n",
      "Requires: scikit-learn, sklearn-crfsuite, ipython, nose, matplotlib, lxml, bqplot, azure-storage, scipy, pdfminer.six, requests, azure-ml-api-sdk, pyspark, docker, tensorflow-gpu, keras, numpy, validators, unidecode, nltk, pytest, pandas, h5py, gensim\n"
     ]
    }
   ],
   "source": [
    "from tatk.feature_extraction.word2vec_vectorizer import Word2VecVectorizer\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "import tatk\n",
    "import collections\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  \n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import num2words\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tatk.pipelines.text_classification.text_classifier import TextClassifier\n",
    "from tatk.pipelines.text_classification.keras_text_classifier import KerasTextClassifier\n",
    "\n",
    "!pip show azureml-tatk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data loading the preprocessing functions for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "# In the following code, we replace Emails, URLS, emoticons etc with special labels\n",
    "pos_emoticons=[\"(^.^)\",\"(^-^)\",\"(^_^)\",\"(^_~)\",\"(^3^)\",\"(^o^)\",\"(~_^)\",\"*)\",\":)\",\":*\",\":-*\",\":]\",\":^)\",\":}\",\n",
    "               \":>\",\":3\",\":b\",\":-b\",\":c)\",\":D\",\":-D\",\":O\",\":-O\",\":o)\",\":p\",\":-p\",\":P\",\":-P\",\":Þ\",\":-Þ\",\":X\",\n",
    "               \":-X\",\";)\",\";-)\",\";]\",\";D\",\"^)\",\"^.~\",\"_)m\",\" ~.^\",\"<=8\",\"<3\",\"<333\",\"=)\",\"=///=\",\"=]\",\"=^_^=\",\n",
    "               \"=<_<=\",\"=>.<=\",\" =>.>=\",\" =3\",\"=D\",\"=p\",\"0-0\",\"0w0\",\"8D\",\"8O\",\"B)\",\"C:\",\"d'-'\",\"d(>w<)b\",\":-)\",\n",
    "               \"d^_^b\",\"qB-)\",\"X3\",\"xD\",\"XD\",\"XP\",\"ʘ‿ʘ\",\"❤\",\"💜\",\"💚\",\"💕\",\"💙\",\"💛\",\"💓\",\"💝\",\"💖\",\"💞\",\n",
    "               \"💘\",\"💗\",\"😗\",\"😘\",\"😙\",\"😚\",\"😻\",\"😀\",\"😁\",\"😃\",\"☺\",\"😄\",\"😆\",\"😇\",\"😉\",\"😊\",\"😋\",\"😍\",\n",
    "               \"😎\",\"😏\",\"😛\",\"😜\",\"😝\",\"😮\",\"😸\",\"😹\",\"😺\",\"😻\",\"😼\",\"👍\"]\n",
    "\n",
    "neg_emoticons=[\"--!--\",\"(,_,)\",\"(-.-)\",\"(._.)\",\"(;.;)9\",\"(>.<)\",\"(>_<)\",\"(>_>)\",\"(¬_¬)\",\"(X_X)\",\":&\",\":(\",\":'(\",\n",
    "               \":-(\",\":-/\",\":-@[1]\",\":[\",\":\\\\\",\":{\",\":<\",\":-9\",\":c\",\":S\",\";(\",\";*(\",\";_;\",\"^>_>^\",\"^o)\",\"_|_\",\n",
    "               \"`_´\",\"</3\",\"<=3\",\"=/\",\"=\\\\\",\">:(\",\">:-(\",\"💔\",\"☹️\",\"😌\",\"😒\",\"😓\",\"😔\",\"😕\",\"😖\",\"😞\",\"😟\",\n",
    "               \"😠\",\"😡\",\"😢\",\"😣\",\"😤\",\"😥\",\"😦\",\"😧\",\"😨\",\"😩\",\"😪\",\"😫\",\"😬\",\"😭\",\"😯\",\"😰\",\"😱\",\"😲\",\n",
    "               \"😳\",\"😴\",\"😷\",\"😾\",\"😿\",\"🙀\",\"💀\",\"👎\"]\n",
    "\n",
    "# Emails\n",
    "emailsRegex=re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "\n",
    "# Mentions\n",
    "userMentionsRegex=re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)')\n",
    "\n",
    "#Urls\n",
    "urlsRegex=re.compile('r(f|ht)(tp)(s?)(://)(.*)[.|/][^ ]+') # It may not be handling all the cases like t.co without http\n",
    "\n",
    "#Numerics\n",
    "numsRegex=re.compile(r\"\\b\\d+\\b\")\n",
    "\n",
    "punctuationNotEmoticonsRegex=re.compile(r'(?<=\\w)[^\\s\\w](?![^\\s\\w])')\n",
    "\n",
    "emoticonsDict = {}\n",
    "for i,each in enumerate(pos_emoticons):\n",
    "    emoticonsDict[each]=' POS_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "for i,each in enumerate(neg_emoticons):\n",
    "    emoticonsDict[each]=' NEG_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "# use these three lines to do the replacement\n",
    "rep = dict((re.escape(k), v) for k, v in emoticonsDict.items())\n",
    "emoticonsPattern = re.compile(\"|\".join(rep.keys()))\n",
    "\n",
    "\n",
    "##########################\n",
    "def read_tweets(filename):\n",
    "    \"\"\"Read the raw tweet data from a file. Replace Emails etc with special tokens \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        all_lines=f.readlines()\n",
    "        padded_lines=[]\n",
    "        for line in all_lines:\n",
    "            line = emoticonsPattern.sub(lambda m: rep[re.escape(m.group(0))], line.lower().strip())\n",
    "            line = userMentionsRegex.sub(' USER ', line )\n",
    "            line = emailsRegex.sub(' EMAIL ', line )\n",
    "            line=urlsRegex.sub(' URL ', line)\n",
    "            line=numsRegex.sub(' NUM ',line)\n",
    "            line=punctuationNotEmoticonsRegex.sub(' PUN ',line)\n",
    "            line=re.sub(r'(.)\\1{2,}', r'\\1\\1',line)\n",
    "            words_tokens=[token for token in TweetTokenizer().tokenize(line)]                    \n",
    "            line= ' '.join(token for token in words_tokens )         \n",
    "            padded_lines.append(line)\n",
    "    return padded_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USER hehe enjoy i say i wish i kept a stock of...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is not happy she is driving a loaner car ! !</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heyy guyzz howz evrionee PUN x</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets    labels\n",
       "0  USER hehe enjoy i say i wish i kept a stock of...  positive\n",
       "1       is not happy she is driving a loaner car ! !  negative\n",
       "2                     heyy guyzz howz evrionee PUN x  positive"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Read in tweet test file. Labels are 4 and 0 (4 being positive and 0 being negative)\n",
    "tweets = read_tweets(test_tweet_filename)\n",
    "\n",
    "with open(test_label_filename, 'r') as f:\n",
    "    all_lines=f.readlines()\n",
    "    tweet_labels = []\n",
    "    for line in all_lines:\n",
    "        line = line.strip()\n",
    "        label = ''\n",
    "        if line == '4':\n",
    "            label = 'positive'\n",
    "        else:\n",
    "            label = 'negative'\n",
    "        tweet_labels.append(label)\n",
    "\n",
    "df_test = pd.DataFrame({'tweets':tweets, 'labels':tweet_labels}, columns=['tweets','labels'])\n",
    "display(df_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate scikit learn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load scikit learn model & pipleine, predict and evaluate the accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseTextModel::load ==> start\n",
      "TatkPipeline::load ==> start\n",
      "Word '<UNK>' is already in vocabulary.\n",
      "Word2VecVectorizer: Word2Vec model loaded from D:\\Sentiment140_Classification\\sk_model.zip 2018-03-17 05.25.22\\sk_model\\pipeline\\tweets_word2vec\\embedding_table.txt\n",
      "Time taken: 0.06 mins\n",
      "TatkPipeline::load ==> end\n",
      "Time taken: 1.68 mins\n",
      "BaseTextModel::load ==> end\n"
     ]
    }
   ],
   "source": [
    "loaded_text_classifier = TextClassifier.load(sk_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassifier::predict ==> start\n",
      "NltkPreprocessor::tatk_transform ==> start\n",
      "NltkPreprocessor::tatk_transform ==> end \t Time taken: 0.09 mins\n",
      "Word2VecVectorizer::tatk_transform ==> start\n",
      "Word2VecVectorizer::tatk_transform ==> end \t Time taken: 0.57 mins\n",
      "VectorAssembler::transform ==> start, num of input records=320000\n",
      "(320000, 50)\n",
      "all_features::\n",
      "(320000, 50)\n",
      "Time taken: 0.0 mins\n",
      "VectorAssembler::transform ==> end\n",
      "LogisticRegression::tatk_predict ==> start\n",
      "LogisticRegression::tatk_predict ==> end \t Time taken: 0.0 mins\n",
      "Time taken: 0.67 mins\n",
      "TextClassifier::predict ==> end\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USER hehe enjoy i say i wish i kept a stock of...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is not happy she is driving a loaner car ! !</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heyy guyzz howz evrionee PUN x</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets    labels prediction\n",
       "0  USER hehe enjoy i say i wish i kept a stock of...  positive   negative\n",
       "1       is not happy she is driving a loaner car ! !  negative   negative\n",
       "2                     heyy guyzz howz evrionee PUN x  positive   positive"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = loaded_text_classifier.predict(df_test)\n",
    "display(predictions[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evalaute the accuracy of the trained text classifier on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassifier::evaluate ==> start\n",
      "NltkPreprocessor::tatk_transform ==> start\n",
      "NltkPreprocessor::tatk_transform ==> end \t Time taken: 0.09 mins\n",
      "Word2VecVectorizer::tatk_transform ==> start\n",
      "Word2VecVectorizer::tatk_transform ==> end \t Time taken: 0.58 mins\n",
      "VectorAssembler::transform ==> start, num of input records=320000\n",
      "(320000, 50)\n",
      "all_features::\n",
      "(320000, 50)\n",
      "Time taken: 0.0 mins\n",
      "VectorAssembler::transform ==> end\n",
      "LogisticRegression::tatk_predict ==> start\n",
      "LogisticRegression::tatk_predict ==> end \t Time taken: 0.0 mins\n",
      "[[117085  43172]\n",
      " [ 41151 118592]]\n",
      "macro_f1 = 0.7364847807033664\n",
      "Time taken: 0.74 mins\n",
      "TextClassifier::evaluate ==> end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[117085,  43172],\n",
       "        [ 41151, 118592]], dtype=int64), 0.7364847807033664)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_text_classifier.evaluate(df_test)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USER hehe enjoy i say i wish i kept a stock of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is not happy she is driving a loaner car ! !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heyy guyzz howz evrionee PUN x</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  labels\n",
       "0  USER hehe enjoy i say i wish i kept a stock of...       1\n",
       "1       is not happy she is driving a loaner car ! !       0\n",
       "2                     heyy guyzz howz evrionee PUN x       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Read in tweet test file. Labels are 4 and 0 (4 being positive and 0 being negative)\n",
    "tweets = read_tweets(test_tweet_filename)\n",
    "\n",
    "with open(test_label_filename, 'r') as f:\n",
    "    all_lines=f.readlines()\n",
    "    tweet_labels = []\n",
    "    for line in all_lines:\n",
    "        line = line.strip()\n",
    "        label = 0\n",
    "        if line == '4':\n",
    "            label = 1\n",
    "        tweet_labels.append(label)\n",
    "\n",
    "df_test = pd.DataFrame({'tweets':tweets, 'labels':tweet_labels}, columns=['tweets','labels'])\n",
    "display(df_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseTextModel::load ==> start\n",
      "TatkPipeline::load ==> start\n",
      "Word2VecVectorizer: Word2Vec model loaded from D:\\Sentiment140_Classification\\keras_model\\pipeline\\vectorizer\\embedding_table.txt\n",
      "Time taken: 0.09 mins\n",
      "TatkPipeline::load ==> end\n",
      "Time taken: 0.09 mins\n",
      "BaseTextModel::load ==> end\n"
     ]
    }
   ],
   "source": [
    "keras_text_classifier_reloaded = KerasTextClassifier.load(keras_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTextClassifier::evaluate ==> start\n",
      "GenerateIndexTransformer::tatk_transform ==> start\n",
      "NltkPreprocessor::tatk_transform ==> startGenerateIndexTransformer::tatk_transform ==> end \t Time taken: 0.0 mins\n",
      "\n",
      "NltkPreprocessor::tatk_transform ==> end \t Time taken: 0.08 mins\n",
      "Word2VecVectorizer::tatk_transform ==> start\n",
      "Word2VecVectorizer::tatk_transform ==> end \t Time taken: 0.09 mins\n",
      "KerasTextClassifierLearner::tatk_predict ==> start\n",
      "KerasTextClassifierLearner::tatk_predict ==> end \t Time taken: 2.92 mins\n",
      "[[121199  39058]\n",
      " [ 24730 135013]]\n",
      "macro_f1=0.8002903328518538\n",
      "Time taken: 3.09 mins\n",
      "KerasTextClassifier::evaluate ==> end\n"
     ]
    }
   ],
   "source": [
    "predictions = keras_text_classifier_reloaded.evaluate(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2bca3e90eb8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix = pd.DataFrame(predictions[0], index = ['pos', 'neg'], columns = ['pos', 'neg'])\n",
    "plt.figure(figsize = (5,3))\n",
    "sns.heatmap(conf_matrix / conf_matrix.sum(axis=1), annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
