{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook shows how to create word vectors using word2vec pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook shows how to generate and save word embeddings from tweets. The tweets are preprocessed to handle URLs, Email ids ets., after which they are input into the word2vec vectorizer.\n",
    "\n",
    "After features are generated, they are saved. They are re-loaded and used for several lookups, such as checking embeddings for words and sentences, examining words which are close to a given word in feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set directories, import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set training file name\n",
    "working_dir = \"D:\\Sentiment140_Classification\"\n",
    "\n",
    "training_filename = os.path.join(working_dir, \"training_text.csv\")\n",
    "embeddings_file_path = os.path.join(working_dir, \"w2vec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: azureml-tatk\n",
      "Version: 0.1.18108.8a1\n",
      "Summary: Microsoft Azure Machine Learning Package for Text Analytics\n",
      "Home-page: https://microsoft.sharepoint.com/teams/TextAnalyticsPackagePreview\n",
      "Author: Microsoft Corporation\n",
      "Author-email: amltap@microsoft.com\n",
      "License: UNKNOWN\n",
      "Location: c:\\users\\remoteuser\\appdata\\local\\amlworkbench\\python\\lib\\site-packages\n",
      "Requires: ipython, pdfminer.six, scikit-learn, h5py, pandas, ruamel.yaml, docker, pyspark, validators, sklearn-crfsuite, ipywidgets, azure-ml-api-sdk, unidecode, scipy, nltk, gensim, nose, azure-storage, qgrid, matplotlib, requests, numpy, pytest, keras, lxml, bqplot\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# Show azureml-tatk version\n",
    "!pip show azureml-tatk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: azureml-tatk\n",
      "Version: 0.1.18108.8a1\n",
      "Summary: Microsoft Azure Machine Learning Package for Text Analytics\n",
      "Home-page: https://microsoft.sharepoint.com/teams/TextAnalyticsPackagePreview\n",
      "Author: Microsoft Corporation\n",
      "Author-email: amltap@microsoft.com\n",
      "License: UNKNOWN\n",
      "Location: c:\\users\\remoteuser\\appdata\\local\\amlworkbench\\python\\lib\\site-packages\n",
      "Requires: qgrid, sklearn-crfsuite, h5py, gensim, azure-ml-api-sdk, nltk, ruamel.yaml, ipython, ipywidgets, pyspark, pdfminer.six, lxml, bqplot, pandas, docker, azure-storage, scipy, pytest, validators, unidecode, numpy, keras, nose, scikit-learn, requests, matplotlib\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  \n",
    "import tensorflow as tf\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import num2words\n",
    "\n",
    "from tatk.feature_extraction.word2vec_vectorizer import Word2VecVectorizer\n",
    "from tatk.feature_extraction.callable_vectorizer import CallableVectorizer\n",
    "\n",
    "!pip show azureml-tatk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions for loading and preprocessing tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "# In the following code, we replace Emails, URLS, emoticons etc with special labels\n",
    "pos_emoticons=[\"(^.^)\",\"(^-^)\",\"(^_^)\",\"(^_~)\",\"(^3^)\",\"(^o^)\",\"(~_^)\",\"*)\",\":)\",\":*\",\":-*\",\":]\",\":^)\",\":}\",\n",
    "               \":>\",\":3\",\":b\",\":-b\",\":c)\",\":D\",\":-D\",\":O\",\":-O\",\":o)\",\":p\",\":-p\",\":P\",\":-P\",\":Þ\",\":-Þ\",\":X\",\n",
    "               \":-X\",\";)\",\";-)\",\";]\",\";D\",\"^)\",\"^.~\",\"_)m\",\" ~.^\",\"<=8\",\"<3\",\"<333\",\"=)\",\"=///=\",\"=]\",\"=^_^=\",\n",
    "               \"=<_<=\",\"=>.<=\",\" =>.>=\",\" =3\",\"=D\",\"=p\",\"0-0\",\"0w0\",\"8D\",\"8O\",\"B)\",\"C:\",\"d'-'\",\"d(>w<)b\",\":-)\",\n",
    "               \"d^_^b\",\"qB-)\",\"X3\",\"xD\",\"XD\",\"XP\",\"ʘ‿ʘ\",\"❤\",\"💜\",\"💚\",\"💕\",\"💙\",\"💛\",\"💓\",\"💝\",\"💖\",\"💞\",\n",
    "               \"💘\",\"💗\",\"😗\",\"😘\",\"😙\",\"😚\",\"😻\",\"😀\",\"😁\",\"😃\",\"☺\",\"😄\",\"😆\",\"😇\",\"😉\",\"😊\",\"😋\",\"😍\",\n",
    "               \"😎\",\"😏\",\"😛\",\"😜\",\"😝\",\"😮\",\"😸\",\"😹\",\"😺\",\"😻\",\"😼\",\"👍\"]\n",
    "\n",
    "neg_emoticons=[\"--!--\",\"(,_,)\",\"(-.-)\",\"(._.)\",\"(;.;)9\",\"(>.<)\",\"(>_<)\",\"(>_>)\",\"(¬_¬)\",\"(X_X)\",\":&\",\":(\",\":'(\",\n",
    "               \":-(\",\":-/\",\":-@[1]\",\":[\",\":\\\\\",\":{\",\":<\",\":-9\",\":c\",\":S\",\";(\",\";*(\",\";_;\",\"^>_>^\",\"^o)\",\"_|_\",\n",
    "               \"`_´\",\"</3\",\"<=3\",\"=/\",\"=\\\\\",\">:(\",\">:-(\",\"💔\",\"☹️\",\"😌\",\"😒\",\"😓\",\"😔\",\"😕\",\"😖\",\"😞\",\"😟\",\n",
    "               \"😠\",\"😡\",\"😢\",\"😣\",\"😤\",\"😥\",\"😦\",\"😧\",\"😨\",\"😩\",\"😪\",\"😫\",\"😬\",\"😭\",\"😯\",\"😰\",\"😱\",\"😲\",\n",
    "               \"😳\",\"😴\",\"😷\",\"😾\",\"😿\",\"🙀\",\"💀\",\"👎\"]\n",
    "\n",
    "emoticonsDict = {}\n",
    "for i,each in enumerate(pos_emoticons):\n",
    "    emoticonsDict[each]=' POS_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "for i,each in enumerate(neg_emoticons):\n",
    "    emoticonsDict[each]=' NEG_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "# use these three lines to do the replacement\n",
    "rep = dict((re.escape(k), v) for k, v in emoticonsDict.items())\n",
    "emoticonsPattern = re.compile(\"|\".join(rep.keys()))\n",
    "\n",
    "# Read in files\n",
    "def read_tweets(filename):\n",
    "    \"\"\"Read the raw tweet data from a file. Replace Emails etc with special tokens \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        all_lines=f.readlines()\n",
    "        padded_lines=[]\n",
    "        for line in all_lines:\n",
    "            line = emoticonsPattern.sub(lambda m: rep[re.escape(m.group(0))], line.lower().strip())\n",
    "            line=re.sub(r'(.)\\1{2,}', r'\\1\\1',line)\n",
    "            words_tokens=[token for token in TweetTokenizer().tokenize(line)]                    \n",
    "            line= ' '.join(token for token in words_tokens )         \n",
    "            padded_lines.append(line)\n",
    "    return padded_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and preprocess tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>damn fixtated on @kokupuff lovely thighs / hip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>god bless firefox's ' restore previous session...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@sherrieshepherd http://twitpic.com/6vn4a - da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@thechadhuck hey ! sorry u had to come back to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bye mommy . we'll miss you .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          raw_tweets\n",
       "0  damn fixtated on @kokupuff lovely thighs / hip...\n",
       "1  god bless firefox's ' restore previous session...\n",
       "2  @sherrieshepherd http://twitpic.com/6vn4a - da...\n",
       "3  @thechadhuck hey ! sorry u had to come back to...\n",
       "4                       bye mommy . we'll miss you ."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets = read_tweets(training_filename)\n",
    "df = pd.DataFrame({'raw_tweets':tweets})\n",
    "display(df[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CallableVectorizer::tatk_fit_transform ==> start\n",
      "CallableVectorizer::tatk_fit_transform ==> end \t Time taken: 0.1 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_tweets</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>damn fixtated on @kokupuff lovely thighs / hip...</td>\n",
       "      <td>damn fixtated on  AT kokupuff lovely thighs / ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>god bless firefox's ' restore previous session...</td>\n",
       "      <td>god bless firefox's ' restore previous session...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@sherrieshepherd http://twitpic.com/6vn4a - da...</td>\n",
       "      <td>AT sherrieshepherd http://twitpic.com/6vn4a -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          raw_tweets  \\\n",
       "0  damn fixtated on @kokupuff lovely thighs / hip...   \n",
       "1  god bless firefox's ' restore previous session...   \n",
       "2  @sherrieshepherd http://twitpic.com/6vn4a - da...   \n",
       "\n",
       "                                              tweets  \n",
       "0  damn fixtated on  AT kokupuff lovely thighs / ...  \n",
       "1  god bless firefox's ' restore previous session...  \n",
       "2   AT sherrieshepherd http://twitpic.com/6vn4a -...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define functions wihch are to be used to pre-process tweets\n",
    "def to_lower_case(x):\n",
    "    return x.lower()\n",
    "\n",
    "def emailsReplace(x):\n",
    "    return x.replace(r'[\\w\\.-]+@[\\w\\.-]+', ' EMAIL ')\n",
    "\n",
    "def numsReplace(x):\n",
    "    return x.replace(r'[\\w\\.-]+@[\\w\\.-]+', ' NUM ')\n",
    "\n",
    "def userMentionsReplace(x):\n",
    "    return x.replace(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)', ' USER ')\n",
    "\n",
    "def urlReplace(x):\n",
    "    return x.replace(r'r(f|ht)(tp)(s?)(://)(.*)[.|/][^ ]+', ' URL ')\n",
    "\n",
    "def punctuationReplace(x):\n",
    "    return x.replace(r'(?<=\\w)[^\\s\\w](?![^\\s\\w])', ' PUN ')\n",
    "\n",
    "def atReplace(x):\n",
    "    return x.replace(r'@', ' AT ')\n",
    "\n",
    "# Chain functions into a list\n",
    "featFuncs=[to_lower_case, emailsReplace, numsReplace, userMentionsReplace, urlReplace, punctuationReplace, atReplace]\n",
    "\n",
    "# Create a transformer specifying functions, \n",
    "callable = CallableVectorizer(input_col=\"raw_tweets\", output_col=\"tweets\", feat_list=featFuncs, preprocessor = True)\n",
    "processed_df = callable.tatk_fit_transform(df)\n",
    "\n",
    "processed_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & train word2vec vectorizer to generate word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::tatk_fit ==> start\n",
      "vocabulary size =69899\n",
      "Word2VecVectorizer::tatk_fit ==> end \t Time taken: 1.05 mins\n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2VecVectorizer(input_col = 'tweets', output_col = '', embedding_size = 50, return_type = 'word_vector', context_window_size=5, min_df=5, num_workers=4)\n",
    "word2vec_model = word2vec.tatk_fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save word2vec embeddings in a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::save_embeddings ==> start\n",
      "Time taken: 0.03 mins\n",
      "Word2VecVectorizer::save_embeddings ==> end\n"
     ]
    }
   ],
   "source": [
    "word2vec_model.save_embeddings(embeddings_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the embeddings to memory with include_unk set to True to add OOV treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::load_embeddings ==> start\n",
      "Time taken: 0.07 mins\n",
      "Word2VecVectorizer::load_embeddings ==> end\n"
     ]
    }
   ],
   "source": [
    "vectorizer = Word2VecVectorizer.load_embeddings(embeddings_file_path, include_unk = True, unk_method = 'rnd', unk_vector = None, unk_word = '<UNK>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Lookup: Get word and subword indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::tatk_transform ==> start\n",
      "Word2VecVectorizer::tatk_transform ==> end \t Time taken: 0.0 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have fever</td>\n",
       "      <td>[5, 23, 1208]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is a good book</td>\n",
       "      <td>[32, 15, 9, 34, 487]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  text               indices\n",
       "0         I have fever         [5, 23, 1208]\n",
       "1  this is a good book  [32, 15, 9, 34, 487]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_predict = pd.DataFrame({'text' : [\"I have fever\", \"this is a good book\"]})\n",
    "vectorizer.input_col = 'text'\n",
    "vectorizer.output_col = 'indices'\n",
    "vectorizer.return_type = 'word_index'\n",
    "result = vectorizer.tatk_transform(df_predict)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Lookup: Get word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::tatk_transform ==> start\n",
      "Word2VecVectorizer::tatk_transform ==> end \t Time taken: 0.0 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>indices</th>\n",
       "      <th>word_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have fever</td>\n",
       "      <td>[5, 23, 1208]</td>\n",
       "      <td>[[-0.014937000349164009, 0.004478000104427338,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is a good book</td>\n",
       "      <td>[32, 15, 9, 34, 487]</td>\n",
       "      <td>[[-0.8164309859275818, -0.9811710119247437, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  text               indices  \\\n",
       "0         I have fever         [5, 23, 1208]   \n",
       "1  this is a good book  [32, 15, 9, 34, 487]   \n",
       "\n",
       "                                         word_vector  \n",
       "0  [[-0.014937000349164009, 0.004478000104427338,...  \n",
       "1  [[-0.8164309859275818, -0.9811710119247437, -0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.output_col = 'word_vector'\n",
    "vectorizer.return_type = 'word_vector'\n",
    "result = vectorizer.tatk_transform(df_predict)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Lookup: Get sentence embedding from word embeddings\n",
    "This uses averaging of word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::tatk_transform ==> start\n",
      "Word2VecVectorizer::tatk_transform ==> end \t Time taken: 0.0 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>indices</th>\n",
       "      <th>word_vector</th>\n",
       "      <th>sentence_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have fever</td>\n",
       "      <td>[5, 23, 1208]</td>\n",
       "      <td>[[-0.014937000349164009, 0.004478000104427338,...</td>\n",
       "      <td>[-0.842507665976882, 0.8129770023127397, 0.927...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is a good book</td>\n",
       "      <td>[32, 15, 9, 34, 487]</td>\n",
       "      <td>[[-0.8164309859275818, -0.9811710119247437, -0...</td>\n",
       "      <td>[-0.3175797998905182, 0.10082738101482391, -1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  text               indices  \\\n",
       "0         I have fever         [5, 23, 1208]   \n",
       "1  this is a good book  [32, 15, 9, 34, 487]   \n",
       "\n",
       "                                         word_vector  \\\n",
       "0  [[-0.014937000349164009, 0.004478000104427338,...   \n",
       "1  [[-0.8164309859275818, -0.9811710119247437, -0...   \n",
       "\n",
       "                                     sentence_vector  \n",
       "0  [-0.842507665976882, 0.8129770023127397, 0.927...  \n",
       "1  [-0.3175797998905182, 0.10082738101482391, -1....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.output_col = 'sentence_vector'\n",
    "vectorizer.return_type = 'sentence_vector'\n",
    "result = vectorizer.tatk_transform(df_predict)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Lookup: Get most similar word to a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bronchitis', 0.7530185580253601),\n",
       " ('tonsillitis', 0.7527911067008972),\n",
       " ('flu', 0.7445870637893677),\n",
       " ('tonsilitis', 0.7410739660263062),\n",
       " ('cough', 0.7255285978317261),\n",
       " ('antibiotics', 0.7244501709938049),\n",
       " ('sickness', 0.7191905975341797),\n",
       " ('sniffles', 0.7161286473274231),\n",
       " ('migraine', 0.7115853428840637),\n",
       " ('chills', 0.6977540850639343)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.embedding_table.most_similar('fever')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
