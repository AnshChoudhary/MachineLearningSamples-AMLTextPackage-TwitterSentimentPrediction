{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import urllib.request\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET FOLDER WHERE DATA WILL DBE DOWNLOADED\n",
    "base_folder = 'Z:\\\\Temp\\\\Sentiment140_Classification'\n",
    "\n",
    "if not os.path.exists(base_folder):\n",
    "    os.makedirs(base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# URL to download the sentiment140 dataset\n",
    "data_url='http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to download and process data\n",
    "\n",
    "def change_base_dir(base_dir_path):\n",
    "    \"\"\" Change the working directopry of the code\"\"\"\n",
    "    \n",
    "    if not os.path.exists(base_dir_path):\n",
    "        print ('creating directory', base_dir_path)\n",
    "        os.makedirs(base_dir_path)\n",
    "    print ('Changing base directory to ', base_dir_path)\n",
    "    os.chdir(base_dir_path)\n",
    "\n",
    "def download_data(download_url, filename='downloaded_data.zip'):\n",
    "    \"\"\" Download and extract data \"\"\"\n",
    "    \n",
    "    downloaded_filename = os.path.join('.', filename)\n",
    "    print ('Step 1: Downloading data')\n",
    "    urllib.request.urlretrieve(download_url,downloaded_filename)\n",
    "    print ('Step 2: Extracting data')\n",
    "    zipfile=ZipFile(downloaded_filename)\n",
    "    zipfile.extractall('./')\n",
    "    zipfile.close()\n",
    "\n",
    "def extract_tweets_and_labels(filename ):\n",
    "    \"\"\" Extract tweets and labels from the downloaded data\"\"\"\n",
    "    \n",
    "    print ('Step 3: Reading the data as a dataframe')\n",
    "    df=pd.read_csv(filename, header=None, encoding='iso-8859-1')    \n",
    "    df.columns=['Label','TweetId','Date','Query','User','Text']\n",
    "    print ('Read {} lines'.format(df.shape[0]))\n",
    "    print ('Discarding neutral tweets')\n",
    "    df=df[df.Label!=2]\n",
    "    print ('No of lines in the data after filtering neutral tweets: {}'.format(df.shape[0]))\n",
    "    print ('Step 4: Shuffling the data')\n",
    "    train_length=int(df.shape[0]*0.8)    \n",
    "    df=df.sample(frac=1) # reshuffling the data\n",
    "      \n",
    "    df['Text']=df['Text'].astype(str).apply(lambda x:x.strip())#.encode('ascii','ignore')#str.decode('utf8','ignore')#.str.encode('ascii','ignore')\n",
    "    print (df.head())\n",
    "    print ('Step 5: Dividing into test and train datasets')\n",
    "    df_train = df.iloc[:train_length, :]\n",
    "    df_test = df.iloc[train_length:, :]    \n",
    "    \n",
    "    print ('Step 6: Exporting the train and test datasets')    \n",
    "    print ('Exporting training data of rows {}'.format(df_train.shape[0]))\n",
    "    export_prefix='training'\n",
    "    df_train[['Label']].to_csv(export_prefix+'_label.csv', header=False, index=False)\n",
    "    df_train[['Text']].to_csv(export_prefix+'_text.csv', header=False, index=False)\n",
    "    print ('Target distribution in the training data is as follows')\n",
    "    print ('\\n',df_train['Label'].value_counts()) \n",
    "    \n",
    "    print ('Exporting training data of rows {}'.format(df_test.shape[0]))\n",
    "    export_prefix='testing'\n",
    "    df_test[['Label']].to_csv(export_prefix+'_label.csv', header=False, index=False)\n",
    "    df_test[['Text']].to_csv(export_prefix+'_text.csv', header=False, index=False)\n",
    "    print ('Target distribution in the testing data is as follows')\n",
    "    print ('\\n',df_test['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing base directory to  Z:\\Temp\\Sentiment140_Classification\n",
      "Step 1: Downloading data\n",
      "Step 2: Extracting data\n",
      "Step 3: Reading the data as a dataframe\n",
      "Read 1600000 lines\n",
      "Discarding neutral tweets\n",
      "No of lines in the data after filtering neutral tweets: 1600000\n",
      "Step 4: Shuffling the data\n",
      "         Label     TweetId                          Date     Query  \\\n",
      "1131141      4  1975815788  Sat May 30 15:57:53 PDT 2009  NO_QUERY   \n",
      "959858       4  1826178296  Sun May 17 07:52:50 PDT 2009  NO_QUERY   \n",
      "1505089      4  2072299085  Sun Jun 07 20:45:41 PDT 2009  NO_QUERY   \n",
      "352926       0  2031407202  Thu Jun 04 09:46:23 PDT 2009  NO_QUERY   \n",
      "513724       0  2190455225  Tue Jun 16 02:46:44 PDT 2009  NO_QUERY   \n",
      "\n",
      "                   User                                               Text  \n",
      "1131141  HannibalKings2  damn fixtated on @kokupuff lovely  thighs/hips...  \n",
      "959858             xSMP  God bless firefox's 'restore previous session'...  \n",
      "1505089  fenderchick130  @SherriEShepherd http://twitpic.com/6vn4a - da...  \n",
      "352926         spaulds1  @TheChadHuck  Hey! Sorry u had to come back to...  \n",
      "513724        missslily                         Bye mommy. We'll miss you.  \n",
      "Step 5: Dividing into test and train datasets\n",
      "Step 6: Exporting the train and test datasets\n",
      "Exporting training data of rows 1280000\n",
      "Target distribution in the training data is as follows\n",
      "\n",
      " 4    640257\n",
      "0    639743\n",
      "Name: Label, dtype: int64\n",
      "Exporting training data of rows 320000\n",
      "Target distribution in the testing data is as follows\n",
      "\n",
      " 0    160257\n",
      "4    159743\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Download and processing the data\n",
    "base_dir_path=base_folder\n",
    "change_base_dir(base_dir_path)\n",
    "download_data(data_url)\n",
    "extract_tweets_and_labels('training.1600000.processed.noemoticon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML Python 3.5",
   "language": "python",
   "name": "aml-python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
